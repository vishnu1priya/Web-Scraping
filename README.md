# Web-Scraping

Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications. There are many different ways to perform web scraping to obtain data from websites. These include using online services, particular APIs or even creating your code for web scraping from scratch. Many large websites, like Google, Twitter, Facebook, StackOverflow, etc. have APIs that allow you to access their data in a structured format. This is the best option, but there are other sites that don’t allow users to access large amounts of data in a structured form or they are simply not that technologically advanced. In that situation, it’s best to use Web Scraping to scrape the website for data.

Web scraping requires two parts, namely the crawler and the scraper. The crawler is an artificial intelligence algorithm that browses the web to search for the particular data required by following the links across the internet. The scraper, on the other hand, is a specific tool created to extract data from the website. The design of the scraper can vary greatly according to the complexity and scope of the project so that it can quickly and accurately extract the data.

Web Scraping eases the task of obtaining and processing data with the help of a structured format. This would help to perform a deeper analysis and answer the aforementioned questions.
So, web scraping helps to fetch such information from websites. Web scraping can be learned in three parts:
1.	Need for and application of web scraping
2.	The basics of an HTML page
3.	Python libraries and codes for web scraping.
Need for web scraping: Some examples
1.	E-commerce: Price comparison
2.	Real estate: To scrap the property details and prices etc.
3.	Social media sites
4.	Stock market: Keep an eye on stocks to fetch maximum profit.



Request library: It is a Python library that is used to read the web page data from the URL of the corresponding page.
BeautifulSoup: It is a Python package that helps in parsing and extracting data from HTML and XML files.
The web scraping process can be divided into four major parts:
1. Reading: For HTML pages read and upload
2. Parsing: For beautifying the HTML code in an understandable format
3. Extraction: For extraction of data from the web page
4. Transformation: For converting the information into the required format, e.g., CSV


# What is Web Scraping used for?

Web Scraping has multiple applications across various industries. Let’s check out some of these now!

1. Price Monitoring
Web Scraping can be used by companies to scrap the product data for their products and competing products as well as to see how it impacts their pricing strategies. Companies can use this data to fix the optimal pricing for their products so that they can obtain maximum revenue.

2. Market Research
Web scraping can be used for market research by companies. High-quality web-scraped data obtained in large volumes can be very helpful for companies in analyzing consumer trends and understanding which direction the company should move in the future. 

3. News Monitoring
Web scraping news sites can provide detailed reports on the current news to a company. This is even more essential for companies that are frequently in the news or that depend on daily news for their day-to-day functioning. After all, news reports can make or break a company in a single day!

4. Sentiment Analysis
If companies want to understand the general sentiment for their products among their consumers, then Sentiment Analysis is a must. Companies can use web scraping to collect data from social media websites such as Facebook and Twitter as to what the general sentiment about their products is. This will help them in creating products that people desire and moving ahead of their competition.

5. Email Marketing
Companies can also use Web scraping for email marketing. They can collect Email IDs from various sites using web scraping and then send bulk promotional and marketing Emails to all the people owning these Email IDs.


